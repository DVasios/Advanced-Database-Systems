{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8595e621",
   "metadata": {},
   "source": [
    "# Advanced Database Systems - NTUA - 2023\n",
    "\n",
    "\n",
    "## Project Scope\n",
    "\n",
    "## \n",
    "\n",
    "### Contributors\n",
    "\n",
    "Dimitris Vasios 03119404\n",
    "\n",
    "Thodoris - Angelos Mexis 03118408"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a63360",
   "metadata": {},
   "source": [
    "### Script to enable the cluster\n",
    "**Cluster Specification**\n",
    "\n",
    "Namenode\n",
    "\n",
    "Datanodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdfbf5e6-14ae-4b72-a1a3-a646a1e3a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, count, when, to_timestamp, udf, rank\n",
    "from pyspark.sql.functions import year, month, count, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Other Libraries\n",
    "import os\n",
    "import subprocess as sp\n",
    "import pandas as pd\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f317561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary data for the project\n",
    "sp.call(['bash', '../scripts/import_data.sh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe07f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Cluster\n",
    "sp.call(['bash', '../scripts/cluster_initiate.sh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Primary Data into one csv\n",
    "crime_data_2010_2019 = pd.read_csv('../data/primary/crime_data_2010_2019.csv')\n",
    "crime_data_2020_present = pd.read_csv('../data/primary/crime_data_2020_present.csv')\n",
    "crime_data = pd.concat([crime_data_2010_2019, crime_data_2020_present], ignore_index=True)\n",
    "crime_data.to_csv('../data/primary/crime_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a379c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data to HDFS\n",
    "sp.call(['bash', '../scripts/load_data_to_hdfs.sh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da57b5",
   "metadata": {},
   "source": [
    "## Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8667758a-80a5-4331-bf2a-d455d43a4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ['SPARK_HOME'] = \"/home/user/opt/spark\"\n",
    "os.environ['HADOOP_HOME'] = \"/home/user/opt/hadoop\"\n",
    "os.environ['PYSPARK PYTHON'] = \"~/envs/venv/bin/python\"\n",
    "\n",
    "# Spark Config\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"AdvDB Project\") \\\n",
    "    .set(\"spark.master\", \"yarn\") \\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .set(\"spark.eventLog.dir\", \"hdfs://okeanos-master:54310/spark.eventLog\") \\\n",
    "    .set(\"spark.history.fs.logDirectory\", \"hdfs://okeanos-master:54310/spark.eventLog\") \\\n",
    "    .set(\"spark.driver.memory\", \"1g\") \\\n",
    "    .set(\"spark.executor.memory\", \"1g\") \\\n",
    "    .set(\"spark.executor.cores\", \"1\") \\\n",
    "    .set(\"spark.yarn.dist.archives\", \"pyspark_venv.tar.gz#venv\") \n",
    "\n",
    "# Start a Spark Session\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de184b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime Data Schema\n",
    "crime_data_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", StringType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType()),\n",
    "])\n",
    "\n",
    "crime_data_df = sc.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crime_data_schema) \\\n",
    "    .load(\"hdfs://okeanos-master:54310/user/data/primary/crime_data.csv\")\n",
    "\n",
    "# Change Columns types\n",
    "crime_data_df = crime_data_df.withColumn('Date Rptd', to_timestamp('Date Rptd', 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "                             .withColumn('DATE OCC', to_timestamp('DATE OCC', 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "                             .withColumn('TIME OCC', col('TIME OCC').cast('int')) \\\n",
    "                             .withColumn('Vict Age', col('Vict Age').cast('int')) \\\n",
    "                             .withColumn('LAT',col('LAT').cast('double')) \\\n",
    "                             .withColumn('LON', col('LON').cast('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe512e",
   "metadata": {},
   "source": [
    "## Print some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0f2f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime Data Total Rows : 2993433\n",
      "Date Rptd: timestamp\n",
      "DATE OCC: timestamp\n",
      "Vict Age: int\n",
      "LAT: double\n",
      "LON: double\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print Total Crime Data Rows\n",
    "rows = crime_data_df.count()\n",
    "print(f\"Crime Data Total Rows : {rows}\")\n",
    "\n",
    "# Print Crime Data Types\n",
    "crime_data_types = crime_data_df.dtypes\n",
    "print(f\"Date Rptd: {crime_data_types[1][1]}\")\n",
    "print(f\"DATE OCC: {crime_data_types[2][1]}\")\n",
    "print(f\"Vict Age: {crime_data_types[11][1]}\")\n",
    "print(f\"LAT: {crime_data_types[26][1]}\")\n",
    "print(f\"LON: {crime_data_types[27][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aadbd318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---+\n",
      "|Year|Month|crimetotal|  #|\n",
      "+----+-----+----------+---+\n",
      "|2010|    3|     17595|  1|\n",
      "|2010|    7|     17520|  2|\n",
      "|2010|    5|     17338|  3|\n",
      "|2011|    8|     17139|  1|\n",
      "|2011|    5|     17050|  2|\n",
      "|2011|    3|     16951|  3|\n",
      "|2012|    8|     17696|  1|\n",
      "|2012|   10|     17477|  2|\n",
      "|2012|    5|     17391|  3|\n",
      "|2013|    8|     17329|  1|\n",
      "|2013|    7|     16714|  2|\n",
      "|2013|    5|     16671|  3|\n",
      "|2014|    7|     14059|  1|\n",
      "|2014|   10|     14031|  2|\n",
      "|2014|    9|     13799|  3|\n",
      "|2015|    8|     18951|  1|\n",
      "|2015|   10|     18916|  2|\n",
      "|2015|    7|     18528|  3|\n",
      "|2016|    8|     19779|  1|\n",
      "|2016|   10|     19615|  2|\n",
      "|2016|    7|     19262|  3|\n",
      "|2017|   10|     20400|  1|\n",
      "|2017|    8|     20086|  2|\n",
      "|2017|    7|     19997|  3|\n",
      "|2018|    5|     20248|  1|\n",
      "|2018|    7|     19972|  2|\n",
      "|2018|   10|     19814|  3|\n",
      "|2019|    7|     19338|  1|\n",
      "|2019|    8|     19074|  2|\n",
      "|2019|    3|     18932|  3|\n",
      "|2020|    1|     18488|  1|\n",
      "|2020|    2|     17436|  2|\n",
      "|2020|    7|     17241|  3|\n",
      "|2021|   11|     23889|  1|\n",
      "|2021|   10|     23882|  2|\n",
      "|2021|   12|     23852|  3|\n",
      "|2022|    5|     21090|  1|\n",
      "|2022|    1|     20845|  2|\n",
      "|2022|    8|     20760|  3|\n",
      "|2023|    8|     20408|  1|\n",
      "|2023|   10|     20385|  2|\n",
      "|2023|    1|     20276|  3|\n",
      "+----+-----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- QUERY 1 | DATAFRAME API ----\n",
    "\n",
    "# Keep specific columns from the dataframe\n",
    "crime_data_date = crime_data_df.select('Date Rptd')\n",
    "\n",
    "# Extract year and month from the 'date_occ' column\n",
    "crime_data_year_month = crime_data_date.withColumn('Year', year('Date Rptd')) \\\n",
    "                                       .withColumn('Month', month('Date Rptd'))\n",
    "\n",
    "# Calculate counts for each year and month\n",
    "counts = crime_data_year_month.groupBy('Year', 'Month').agg(count('*').alias('crimetotal'))\n",
    "\n",
    "# Order by Year and Total Crimes Crimes\n",
    "partitioned = Window.partitionBy('Year').orderBy(counts['crimetotal'].desc())\n",
    "\n",
    "# Add a rank column to the DataFrame\n",
    "ranked_df = counts.withColumn('rnk', dense_rank().over(partitioned))\n",
    "\n",
    "# Filter the top 3 counts for each year\n",
    "top3_df = ranked_df.filter('rnk <= 3')\n",
    "\n",
    "# Rename the rank column\n",
    "top3 = top3_df.withColumnRenamed('rnk', '#')\n",
    "\n",
    "# Show the results\n",
    "top3.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Query 1 | SQL API ----\n",
    "query_1_sql = \"\"\" with MonthlyCrimeCounts AS ( \n",
    "  SELECT  \n",
    "    EXTRACT(YEAR FROM `Date Rptd`) AS Year, \n",
    "    EXTRACT(MONTH FROM `Date Rptd`) AS Month, \n",
    "    COUNT(*) AS crime_count,  \n",
    "    ROW_NUMBER() OVER (PARTITION BY EXTRACT(YEAR FROM `Date Rptd`) ORDER BY COUNT(*) DESC) AS rn \n",
    "  FROM \n",
    "    crime_data\n",
    "  GROUP BY \n",
    "    Year, \n",
    "    Month \n",
    ") \n",
    "\n",
    "SELECT \n",
    "  Year, \n",
    "  Month, \n",
    "  crime_count, \n",
    "  rn AS month_rank  \n",
    "FROM \n",
    "  MonthlyCrimeCounts \n",
    "WHERE \n",
    "  rn <= 3 \n",
    "ORDER BY \n",
    "  Year ASC, \n",
    "  crime_count DESC; \"\"\"\n",
    "\n",
    "crime_data_df.createOrReplaceTempView(\"crime_data\")\n",
    "crime_data_query_1 = sc.sql(query_1_sql)\n",
    "crime_data_query_1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd4ee8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|PartOfDay|NumberOfCrimes|\n",
      "+---------+--------------+\n",
      "|    Night|        237605|\n",
      "|Afternoon|        187306|\n",
      "|     Noon|        148180|\n",
      "|  Morning|        123846|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ---- Query 2 | Dataframe API ----\n",
    "\n",
    "query_2 = crime_data_df \\\n",
    "    .filter(crime_data_df['Premis Desc'] == 'STREET') \\\n",
    "    .withColumn( \n",
    "        'PartOfDay', \n",
    "        when((crime_data_df['TIME OCC'] >= 500) & (crime_data_df['TIME OCC'] < 1200), 'Morning') \\\n",
    "        .when((crime_data_df['TIME OCC'] >= 1200) & (crime_data_df['TIME OCC'] < 1700), 'Noon') \\\n",
    "        .when((crime_data_df['TIME OCC'] >= 1700) & (crime_data_df['TIME OCC'] < 2100), 'Afternoon') \\\n",
    "        .when((crime_data_df['TIME OCC'] >= 2100) & (crime_data_df['TIME OCC'] < 2400) |\n",
    "              (crime_data_df['TIME OCC'] >= 0) & (crime_data_df['TIME OCC'] < 500), 'Night') \\\n",
    "        .otherwise('NoPartOfDay')) \\\n",
    "    .select(col('TIME OCC').alias('time'), col('PartOfDay')) \\\n",
    "    .groupBy(col('PartOfDay')).agg(count('*').alias('NumberOfCrimes')) \\\n",
    "    .orderBy(col('NumberOfCrimes').desc()) \n",
    "\n",
    "# Print Output\n",
    "query_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ada9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Query 2 | SQL API\n",
    "\n",
    "crime_data_df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "query_2_sql = \"\"\"\n",
    "\n",
    "WITH OnlyInStreet AS (\n",
    "  SELECT \n",
    "    `TIME OCC` as time,\n",
    "    CASE \n",
    "      WHEN time >= 500 AND time < 1200 THEN 'Morning'\n",
    "      WHEN time >= 1200 AND time < 1700 THEN 'Noon' \n",
    "      WHEN time >= 1700 AND time < 2100 THEN 'Afternoon' \n",
    "      WHEN time >= 2100 AND time < 2400 OR time >=0 AND time < 500 THEN 'Night' \n",
    "    END AS PartOfDay\n",
    "  FROM\n",
    "    crime_data\n",
    "  WHERE\n",
    "    `Premis Desc`='STREET'\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  PartOfDay,\n",
    "  COUNT(PartOFDay) as NumberOfCrimes\n",
    "FROM\n",
    "  OnlyInStreet\n",
    "GROUP BY\n",
    "  PartOfDay\n",
    "ORDER BY\n",
    "  NumberOfCrimes DESC;\n",
    "\"\"\"\n",
    "\n",
    "crime_data_query = sc.sql(query_2_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Dataframe\n",
    "rdd_from_df = crime_data_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56552bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Query 2 | RDD API ----\n",
    "\n",
    "def categorize(x): \n",
    "    if x[0] >= 500 and x[0] < 1200:\n",
    "        return ('Morning', 1)\n",
    "    elif x[0] >= 1200 and x[0] < 1700:\n",
    "        return ('Noon', 1)\n",
    "    elif x[0] >= 1700 and x[0] < 2100:\n",
    "        return ('Afternoon', 1)\n",
    "    elif (x[0] >= 2100 and x[0] < 2400) or (x[0] >= 0 and x[0] < 500):\n",
    "        return ('Night', 1)\n",
    "        \n",
    "# rdd = sc.textFile(\"hdfs://okeanos-master:54310/user/data/primary/crime_data.csv\") \n",
    "# rdd = sc.textFile(\"../data/primary/crime_data.csv\") \n",
    "\n",
    "data_rdd = rdd_from_df \\\n",
    "                .map(lambda x: (int(x[3]), str(x[15]))) \\\n",
    "                .filter(lambda x: x[1] == \"STREET\") \\\n",
    "                .map(categorize) \\\n",
    "                .reduceByKey(lambda x, y: x+y) \n",
    "\n",
    "result = data_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe27f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'PartOfDay', 'NumberOfCrimes']\n",
    "rows = []\n",
    "for i in result: \n",
    "    rows.append([i[0], i[1]])\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "print(df.sort_values(by='NumberOfCrimes', ascending=False, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df8b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Secondary Data --- Median Income & Revgecoding\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------- |\n",
    "\n",
    "# Median Income 2015 Schema\n",
    "median_income_schema = StructType([\n",
    "    StructField(\"Zip Code\", StringType()),\n",
    "    StructField(\"Community\", StringType()),\n",
    "    StructField(\"Estimated Median Income\", StringType())\n",
    "])\n",
    "\n",
    "# Load data from dfs\n",
    "median_income_df = sc \\\n",
    "    .read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(median_income_schema) \\\n",
    "    .load(\"hdfs://okeanos-master:54310/user/data/secondary/median_household_incomes/LA_income_2015.csv\")\n",
    "\n",
    "# Function to alter 'estimated median income' to integer | $XX,YYY -> XXYYY\n",
    "alter_median_income_col = udf(lambda x: int(x.replace('$', '').replace(',', '')))\n",
    "\n",
    "# Change Column Types & Drop Duplicates\n",
    "median_income_df = median_income_df \\\n",
    "    .withColumn(\"Zip Code\", col(\"Zip Code\").cast('int')) \\\n",
    "    .withColumn(\"Estimated Median Income\", alter_median_income_col(col(\"Estimated Median Income\"))) \\\n",
    "    .drop_duplicates([\"Zip Code\"]) \\\n",
    "    .select(col(\"Zip Code\").alias(\"zipcode\"), col(\"Estimated Median Income\").alias('median_income'))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------- |\n",
    "\n",
    "# Revgecoding Schema\n",
    "revgecoding_schema = StructType([\n",
    "    StructField(\"LAT\", StringType()),\n",
    "    StructField(\"LON\", StringType()),\n",
    "    StructField(\"ZIPcode\", StringType()),\n",
    "])\n",
    "\n",
    "# Load Data from DFS\n",
    "revgecoding_df =  sc \\\n",
    "    .read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(revgecoding_schema) \\\n",
    "    .load(\"hdfs://okeanos-master:54310/user/data/secondary/revgecoding.csv\")\n",
    "\n",
    "# Change Column Types\n",
    "revgecoding_df = revgecoding_df \\\n",
    "    .withColumn(\"LAT\", col(\"LAT\").cast(\"double\")) \\\n",
    "    .withColumn(\"LON\", col(\"LON\").cast(\"double\")) \\\n",
    "    .withColumn(\"ZIPcode\", col(\"ZIPcode\").cast(\"int\")) \\\n",
    "    .select(col(\"LAT\"), col(\"LON\"), col(\"ZIPcode\").alias(\"zipcode\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe20ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Secondary Dataframes to the main\n",
    "income_from_coordinates_df = median_income_df.join(revgecoding_df, on=\"zipcode\")\n",
    "\n",
    "joined_crime_data_df = crime_data_df \\\n",
    "    .join(income_from_coordinates_df, (crime_data_df[\"LAT\"] == income_from_coordinates_df[\"LAT\"]) & (crime_data_df[\"LON\"] == income_from_coordinates_df[\"LON\"])) \\\n",
    "    .select(col('Vict Descent').alias('descent'), col('median_income'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d91fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for Descent\n",
    "descent_dict = {\n",
    "    'A': 'Other Asian',\n",
    "    'B': 'Black',\n",
    "    'C': 'Chinese',\n",
    "    'D': 'Cambodian',\n",
    "    'F': 'Filipino',\n",
    "    'G': 'Guamanian',\n",
    "    'H': 'Hispanic/Latin/Mexican',\n",
    "    'I': 'American Indian/Alaskan Native',\n",
    "    'J': 'Japanese',\n",
    "    'K': 'Korean',\n",
    "    'L': 'Laotian',\n",
    "    'O': 'Other',\n",
    "    'P': 'Pacific Islander',\n",
    "    'S': 'Samoan',\n",
    "    'U': 'Hawaiian',\n",
    "    'V': 'Vietnamese',\n",
    "    'W': 'White',\n",
    "    'X': 'Unknown',\n",
    "    'Z': 'Asian Indian'\n",
    "}\n",
    "\n",
    "get_descent = udf(lambda x: descent_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a926ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|             descent|  #|\n",
      "+--------------------+---+\n",
      "|               White| 73|\n",
      "|               Other|  9|\n",
      "|Hispanic/Latin/Me...|  8|\n",
      "|             Unknown|  3|\n",
      "|               Black|  2|\n",
      "|         Other Asian|  1|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:32:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:32:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             descent|   #|\n",
      "+--------------------+----+\n",
      "|               White|1927|\n",
      "|               Other| 698|\n",
      "|             Unknown| 534|\n",
      "|Hispanic/Latin/Me...| 369|\n",
      "|               Black| 265|\n",
      "|         Other Asian| 141|\n",
      "|             Chinese|  16|\n",
      "|              Korean|   7|\n",
      "|            Japanese|   5|\n",
      "|        Asian Indian|   4|\n",
      "|           Guamanian|   1|\n",
      "|          Vietnamese|   1|\n",
      "|            Hawaiian|   1|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:33:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             descent|   #|\n",
      "+--------------------+----+\n",
      "|               White|5556|\n",
      "|               Other|1714|\n",
      "|Hispanic/Latin/Me...|1155|\n",
      "|               Black| 734|\n",
      "|             Unknown| 622|\n",
      "|         Other Asian| 237|\n",
      "|            Filipino|  19|\n",
      "|American Indian/A...|   9|\n",
      "|        Asian Indian|   5|\n",
      "|              Korean|   4|\n",
      "|            Japanese|   4|\n",
      "|             Chinese|   3|\n",
      "|    Pacific Islander|   2|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:33:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:33:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             descent|   #|\n",
      "+--------------------+----+\n",
      "|               White|5301|\n",
      "|Hispanic/Latin/Me...|1400|\n",
      "|               Other|1286|\n",
      "|             Unknown|1131|\n",
      "|               Black| 471|\n",
      "|         Other Asian| 242|\n",
      "|            Filipino|  19|\n",
      "|             Chinese|   8|\n",
      "|American Indian/A...|   7|\n",
      "|        Asian Indian|   7|\n",
      "|              Korean|   5|\n",
      "|            Japanese|   5|\n",
      "|          Vietnamese|   3|\n",
      "|    Pacific Islander|   2|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:34:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             descent|   #|\n",
      "+--------------------+----+\n",
      "|               White|5557|\n",
      "|Hispanic/Latin/Me...| 914|\n",
      "|               Other| 834|\n",
      "|             Unknown| 783|\n",
      "|               Black| 723|\n",
      "|         Other Asian| 291|\n",
      "|              Korean|  10|\n",
      "|             Chinese|   9|\n",
      "|            Filipino|   4|\n",
      "|            Japanese|   4|\n",
      "|American Indian/A...|   2|\n",
      "|            Hawaiian|   2|\n",
      "|          Vietnamese|   1|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:34:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:34:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:35:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/17 15:35:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|             descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|677|\n",
      "|               Other|310|\n",
      "|Hispanic/Latin/Me...|181|\n",
      "|         Other Asian|116|\n",
      "|               Black| 54|\n",
      "|             Unknown| 50|\n",
      "|             Chinese|  5|\n",
      "|              Korean|  5|\n",
      "|            Filipino|  4|\n",
      "|            Japanese|  3|\n",
      "|American Indian/A...|  2|\n",
      "|          Vietnamese|  1|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----- Query 3 | Dataframe API\n",
    "query_3_filtered = joined_crime_data_df \\\n",
    "    .filter((col(\"descent\").isNotNull()) & (col(\"descent\") != '')) \n",
    "\n",
    "# Ranked Desc\n",
    "window_spec_desc = Window.orderBy(col(\"median_income\").desc())\n",
    "ranked_desc_df = query_3_filtered \\\n",
    "    .withColumn(\"Rank\", dense_rank().over(window_spec_desc)) \n",
    "\n",
    "# Ranked Asc\n",
    "window_spec_asc = Window.orderBy(col(\"median_income\").asc())\n",
    "ranked_asc_df = query_3_filtered \\\n",
    "    .withColumn(\"Rank\", dense_rank().over(window_spec_asc))\n",
    "\n",
    "# Query\n",
    "def query_3 (ranked, num): \n",
    "    return ranked \\\n",
    "    .filter(col(\"Rank\") == num) \\\n",
    "    .groupBy(\"descent\").agg(count('*').alias(\"#\")) \\\n",
    "    .withColumn(\"descent\", get_descent(col(\"descent\"))) \\\n",
    "    .orderBy(col(\"#\").desc())\n",
    "\n",
    "# Print first 3 \n",
    "for i in range(1,4):\n",
    "    q = query_3(ranked_desc_df, i)\n",
    "    q.show()\n",
    "\n",
    "# Print last 3 \n",
    "for i in range(1,4):\n",
    "    q = query_3(ranked_asc_df, i)\n",
    "    q.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Query 3 | SQL API\n",
    "\n",
    "joined_crime_data_df.createOrReplaceTempView(\"joined_crime_data\")\n",
    "\n",
    "def query_3_sql(num, type):\n",
    "  return f\"\"\"\n",
    "\n",
    "  with ranked as (\n",
    "    select \n",
    "      descent,\n",
    "      median_income,\n",
    "      dense_rank() over (order by `median_income` {type}) as rank\n",
    "    from joined_crime_data\n",
    "    where \n",
    "      descent is not null and trim(descent) != ''\n",
    "  ) \n",
    "\n",
    "  select \n",
    "    descent,\n",
    "    count(`median_income`) as rnk\n",
    "  from \n",
    "    ranked\n",
    "  where\n",
    "    rank={num}\n",
    "  group by descent\n",
    "  order by rnk desc;\n",
    "\"\"\"\n",
    "\n",
    "# Print first three\n",
    "for i in range(1,4):\n",
    "  sc.sql(query_3_sql(i, 'desc')).show()\n",
    "\n",
    "# Print last three\n",
    "for i in range(1,4):\n",
    "  sc.sql(query_3_sql(i, 'desc')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf9f963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process Secondary Police Stations Data\n",
    "\n",
    "#  Median Income 2015 Schema\n",
    "police_stations_schema = StructType([\n",
    "    StructField(\"X\", StringType()),\n",
    "    StructField(\"Y\", StringType()),\n",
    "    StructField(\"FID\", StringType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", StringType())\n",
    "])\n",
    "\n",
    "# Load data from dfs\n",
    "police_stations_df = sc \\\n",
    "    .read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(police_stations_schema) \\\n",
    "    .load(\"hdfs://okeanos-master:54310/user/data/secondary/LAPD_Police_Stations.csv\")\n",
    "\n",
    "# Change Column Types and Select with aliased names\n",
    "police_stations_df = police_stations_df \\\n",
    "    .withColumn(\"X\", col(\"X\").cast(\"double\")) \\\n",
    "    .withColumn(\"Y\", col(\"Y\").cast(\"double\")) \\\n",
    "    .select(col(\"X\").alias(\"police_station_lat\"), col(\"Y\").alias(\"police_station_lon\"), col(\"PREC\").alias(\"police_station\")) \n",
    "\n",
    "# Join Data\n",
    "joined_police_station_df = crime_data_df \\\n",
    "    .withColumn(\"AREA\", col(\"AREA\").cast('int')) \\\n",
    "    .select(col(\"Weapon Used Cd\").alias(\"weapon\"), col(\"LAT\").alias(\"crime_lat\"), col(\"LON\").alias(\"crime_lon\"), col(\"AREA\").alias(\"police_station\")) \\\n",
    "    .join(police_stations_df, on=\"police_station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77e9bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Distance Function\n",
    "def get_distance_geopy (lat1, lon1, lat2, lon2):\n",
    "    return geopy.distance.geodesic((lat1, lon2), (lat2, lon2))\n",
    "\n",
    "get_distance = udf(get_distance_geopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1be2c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:35:37 WARN TaskSetManager: Lost task 0.0 in stage 61.0 (TID 97) (okeanos-master executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'geopy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/01/17 15:35:38 ERROR TaskSetManager: Task 0 in stage 61.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'geopy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---- Query 4 | Dataframe API ----\u001b[39;00m\n\u001b[1;32m      3\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m  joined_police_station_df \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(joined_police_station_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweapon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNULL\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, get_distance(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrime_lat\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrime_lon\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolice_station_lat\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolice_station_lon\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/envs/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/envs/venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1705486182927_0023/container_1705486182927_0023_01_000003/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'geopy'\n"
     ]
    }
   ],
   "source": [
    "# ---- Query 4 | Dataframe API ----\n",
    "\n",
    "filtered_df =  joined_police_station_df \\\n",
    "    .filter(joined_police_station_df['weapon'] != 'NULL') \\\n",
    "    .withColumn(\"Distance\", get_distance(col(\"crime_lat\"), col(\"crime_lon\"), col(\"police_station_lat\"), col(\"police_station_lon\")))\n",
    "\n",
    "filtered_df.limit(100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Query 4 | SQL API\n",
    "\n",
    "\n",
    "# Σύνδεση μεταξύ των τμημάτων και των εγκλημάτων\n",
    "joined_df = crime_df.join(police_station_df, col(\"AREA\") == col(\"PRECINCT\"), \"inner\")\n",
    "\n",
    "# 3a : Year\n",
    "query_3a = \"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(YEAR FROM to_date(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a')) AS Year,\n",
    "        COUNT(*) AS CrimeCount,\n",
    "        AVG(distance_from_police) AS AvgDistance\n",
    "    FROM (\n",
    "        SELECT\n",
    "            `DATE OCC`,\n",
    "            CASE WHEN `Crm Cd 1` LIKE '1%' OR `Crm Cd 2` LIKE '1%' OR `Crm Cd 3` LIKE '1%' OR `Crm Cd 4` LIKE '1%' THEN 1 ELSE 0 END AS is_firearm_used,\n",
    "            ST_DISTANCE(ST_POINT(`LON`, `LAT`), ST_POINT(`police_lon`, `police_lat`)) AS distance_from_police\n",
    "        FROM joined_data\n",
    "        WHERE `LON` IS NOT NULL AND `LAT` IS NOT NULL\n",
    "    ) AS subquery\n",
    "    GROUP BY Year\n",
    "    ORDER BY Year\n",
    "\"\"\"\n",
    "\n",
    "result_a = spark.sql(query_3a)\n",
    "\n",
    "# 3b : Police Station\n",
    "query_3b = \"\"\"\n",
    "    SELECT\n",
    "        `PRECINCT`,\n",
    "        COUNT(*) AS CrimeCount,\n",
    "        AVG(distance_from_police) AS AvgDistance\n",
    "    FROM (\n",
    "        SELECT\n",
    "            `PRECINCT`,\n",
    "            CASE WHEN `Crm Cd 1` LIKE '1%' OR `Crm Cd 2` LIKE '1%' OR `Crm Cd 3` LIKE '1%' OR `Crm Cd 4` LIKE '1%' THEN 1 ELSE 0 END AS is_firearm_used,\n",
    "            ST_DISTANCE(ST_POINT(`LON`, `LAT`), ST_POINT(`police_lon`, `police_lat`)) AS distance_from_police\n",
    "        FROM joined_data\n",
    "        WHERE `LON` IS NOT NULL AND `LAT` IS NOT NULL\n",
    "    ) AS subquery\n",
    "    GROUP BY `PRECINCT`\n",
    "    ORDER BY CrimeCount DESC\n",
    "\"\"\"\n",
    "\n",
    "result_b = spark.sql(query_3b)\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων\n",
    "print(\"per Year\")\n",
    "result_a.show()\n",
    "\n",
    "print(\"per Police Station\")\n",
    "result_b.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
